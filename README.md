# ğŸš€ GenAI Training Projects  

This repository showcases a collection of projects developed as part of the **Generative AI Training Program** conducted in our college.  
The projects explore **Large Language Models (LLMs), Computer Vision, Gesture Control, and Visionâ€“Language Model integrations**.  

---

## ğŸ‘¨â€ğŸ’» Team Members  
- ğŸ‘¤ Jeeva  
- ğŸ‘¤ Jaizil Antony  
- ğŸ‘¤ Sajan  
- ğŸ‘¤ Kavin  
- ğŸ‘¤ Karthikeyan  

---

## ğŸ“‚ Projects Overview  

### ğŸ¤– 1. AI Chatbot (Groq API)  
A console-based chatbot powered by **Groqâ€™s LLMs**.  

ğŸ”¹ **Features**  
- Real-time interactive Q&A.  
- Uses Groq API for intelligent responses.  
- Minimal and simple terminal interface.  

ğŸ“Œ **Use Case**: Ideal for experimenting with large language models, quick Q&A assistance, and educational demonstrations.  

---

### âœ‹ 2. Hand Gesture Recognition for Driving Simulation  
A **gesture-based driving control system** using **computer vision**.  

ğŸ”¹ **Features**  
- Tracks **left-hand finger counts** using a webcam.  
- Controls a driving simulator with gestures:  
  - âœ‹ 5 fingers â†’ **Accelerate**  
  - âœŠ 0 fingers â†’ **Brake**  
  - ğŸ¤Ÿ 3 fingers â†’ **Neutral**  
- Built with **OpenCV + cvzone + PyAutoGUI**.  

ğŸ“Œ **Use Case**: Can be used in simulators, training modules, or accessibility-focused applications where physical controls are limited.  

---

### ğŸ‘ï¸ 3. Live Vision â†’ LLM (Ollama Integration)  
Connects a **live camera feed** with a **vision-enabled LLM** to enable real-time scene understanding.  

ğŸ”¹ **Features**  
- Captures frames at **24 FPS**.  
- Users can ask context-based questions such as:  
  - *â€œWhat do you see?â€*  
  - *â€œWhat dish can I make with these ingredients?â€*  
- Supports **Ollama Python Client** and **REST API fallback**.  

ğŸ“Œ **Use Case**: Useful in real-time analysis, object detection, cooking assistance, and AI-based perception tasks.  

---

## ğŸ› ï¸ Tech Stack  
- ğŸ **Language**: Python  
- âš¡ **APIs & Libraries**: Groq API, Ollama, OpenCV, cvzone, PyAutoGUI, Requests, NumPy  

---

## âš™ï¸ Installation & Setup  

### ğŸ”½ 1. Clone the Repository  
```bash
git clone https://github.com/<your-username>/GenAI-Training-Projects.git
cd GenAI-Training-Projects
```

### ğŸŒ± 2. Create a Virtual Environment *(Optional but Recommended)*  
```bash
python -m venv env
```
- **Linux/Mac:** `source env/bin/activate`  
- **Windows:** `env\Scripts\activate`  

### ğŸ“¦ 3. Install Dependencies  
```bash
pip install -r requirements.txt
```

### ğŸ”‘ 4. Additional Setup  
- **AI Chatbot:** Obtain a **Groq API Key** from [Groq Console](https://console.groq.com/) and update the configuration.  
- **Live Vision â†’ LLM:** Install and run [Ollama](https://ollama.ai), then download the vision model:  
  ```bash
  ollama pull llama3.2-vision
  ```  

---

## â–¶ï¸ Running the Projects  

- ğŸ¤– **AI Chatbot** â†’ Start the chatbot script and interact with the LLM in your terminal.  
- âœ‹ **Hand Gesture Driving Control** â†’ Run the gesture recognition script and control a simulation with hand signs.  
- ğŸ‘ï¸ **Live Vision â†’ LLM** â†’ Launch the live vision script and ask the model about what the camera sees.  

---

## ğŸ“˜ Report  
A detailed project report is available in [`REPORT.md`](./REPORT.md).  

---

## ğŸ“œ License  
This repository is for **educational purposes only** as part of the Generative AI training program.  
Feel free to use and adapt it with proper attribution.  
